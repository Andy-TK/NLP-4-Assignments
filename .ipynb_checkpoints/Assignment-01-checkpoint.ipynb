{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`各位同学大家好，欢迎各位开始学习我们的人工智能课程。这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。根据往期同学的实际反馈，我们课程的完结之后 能力能够超过80%的计算机人工智能/深度学习方向的硕士生的能力。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 请回答以下问题\n",
    "\n",
    "回答以下问题，并将问题发送至 mqgao@kaikeba.com中：\n",
    "```\n",
    "    2.1. what do you want to acquire in this course？\n",
    "    2.2. what problems do you want to solve？\n",
    "    2.3. what’s the advantages you have to finish you goal?\n",
    "    2.4. what’s the disadvantages you need to overcome to finish you goal?\n",
    "    2.5. How will you plan to study in this course period?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 如何提交\n",
    "代码 + 此 jupyter 相关，提交至自己的 github 中(**所以请务必把GitHub按照班主任要求录入在Trello中**)；\n",
    "第2问，请提交至mqgao@kaikeba.com邮箱。\n",
    "#### 4. 作业截止时间\n",
    "此次作业截止时间为 2019.7.6日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammar(grammar_str, split='=>'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split('\\n'):\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        exp, stmt = line.split(split)\n",
    "        grammar[exp.strip()] = [s.split() for s in stmt.split('|')]\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(gram, target):\n",
    "    if target not in gram:  # means target is a terminal expression\n",
    "        return target\n",
    "    expand = [generate(gram, t) for t in random.choice(gram[target])]\n",
    "    return ''.join(e for e in expand if e != 'null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "1. Autonomous Driving Cars  \n",
    "2. Commercial Advertisement Pushing  \n",
    "3. Intelligent Voice Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:   \n",
    "1. Github: We use Github to manage our code and works with version control.  \n",
    "Main commands:  \n",
    "Clone an existing repository: git clone &lt;url_of_repository&gt;  \n",
    "Create a new local repository: git init  \n",
    "Add all current changes to the next commit: git add .  \n",
    "Commit previously staged changes: git commit -m \"&lt;modify&gt;\"  \n",
    "Switch HEAD branch: git checkout &lt;branch&gt;  \n",
    "Create a new branch based on your current HEAD: git branch &lt;new-branch&gt;  \n",
    "Delete a local branch: git branch -d &lt;branch&gt;  \n",
    "Mark the current commit with a tag: git tag &lt;tag-name&gt;  \n",
    "Download all changes from &lt;remote&gt;, but don’t integrate into HEAD: git fetch &lt;remote&gt;  \n",
    "Download changes and directly merge/integrate into HEAD: git pull &lt;remote&gt; &lt;branch&gt;  \n",
    "Publish local changes on a remote: git push &lt;remote&gt; &lt;branch&gt;  \n",
    "Delete a branch on the remote: git branch -dr &lt;remote/branch&gt;  \n",
    "Publish your tags: git push --tags  \n",
    "Merge &lt;branch&gt; into your current HEAD: git merge &lt;branch&gt;  \n",
    "2. Jupyter: We can use Jupyter as a notebook. Also, it provides a lab environment for coding with cells.  \n",
    "3. Pycharm: It provides a Python IDE for developing big projects with Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "A probability model is a mathematical representation of a random phenomenon.  \n",
    "It is defined by its sample space, events within the sample space, and probabilities associated with each event.  \n",
    "$S$: Sample Spaces, which represents the set of all possible outcomes.  \n",
    "$A$: An event, which is a subset of the Sample Space $S$  \n",
    "$P(A)$: The probability of any event $A$ out of $S$.  \n",
    "$$P(A) = Count(A)/Count(S)$$    \n",
    "\n",
    "The probability model follows some basic rules:  \n",
    "1. Any probability $P(A)$ is a number between $0$ and $1$ $( 0 < P(A) < 1 )$.  \n",
    "2. The probability of the sample space $S$ is equal to $1$ $( P(S) = 1 )$.  \n",
    "3. If two events $A$ and $B$ are disjoint, then the probability of either event is the sum of the probabilities of the two events:  \n",
    "$P(A \\cup B) = P(A) + P(B)$  \n",
    "4. The probability that any event $A$ does not occur is: $P(A^c) = 1 - P(A)$  \n",
    "5. If two events $A$ and $B$ are independent, the probability of both events is the product of the probabilities for each event:  \n",
    "$P(A \\cap B) = P(A)P(B)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "Suppose a specific protein is related to cancer. We have collected some relative data from past patients. Wirh Probability Model, we can make a prediction for a new patient by giving a probability whether the patient has a cancer or not. Also, it provides a theoretical support for some machine learning algorithms (e.g. Navie Bayes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "Syntactic-based methods have dominated the field of NLP for a long time. However, this type of method has the following drawbacks:  \n",
    "1. In the traditional linguistic approach, the process of splitting the syntax tree is usually very complicated.  \n",
    "2. Due to the flexibility of human natural language, the generalization ability of grammar-based methods is poor, which means that although we can generate sentences by defining grammar, it is difficult to reverse the grammar according to sentences.  \n",
    "3. It is difficult to choose the most appropriate sentence among a set of candidate sentences. For example, there could be several candidate sentences with correct grammar, however, some of them might be strange or seems to be not natural from the view of human beings. \n",
    "  \n",
    "By introducing probability theory, we can construct a Language Model to solve this problem. This method performs quite well in many cases which need a decision, such as speech recognition and Q&A system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length $m$, it assigns a probability $P(w_1,... ,w_m)$ to the whole sequence.  \n",
    "According to the Chain Rule of Conditional Probability, it can be denoted as:  \n",
    "$$P(w_1,... ,w_m)=P(w_1)P(w_2 \\mid w_1)...P(w_m \\mid w_1,...,w_{m-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "1. Speech recognition: different word sequences can have the same pronunciation, we can judge it through language model.  \n",
    "2. Sentence generation: for example, when we design a dialogue system, suppose there are many sentences that can be responded to, we can use the language model to select the most appropriate sentence in the grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "Data sparsity is a major problem in building language models. Most possible word sequences are not observed in training. One solution is introducing the Markov Assumption, which assume that the probability of the current word only depends on the previous n-1 words. This is known as the n-gram model. In an n-gram model, the probability $P(w_1,... ,w_m)$ of observing the sentence $w_1,... ,w_m$ is approximated as:  \n",
    "\n",
    "$$P(w_1,... ,w_m)=\\prod_{i=1}^{m} P(w_i \\mid w_1,...,w_{i-1})\\approx \\prod_{i=1}^{m} P(w_i \\mid w_{i-(n-1)},...,w_{i-1})$$\n",
    "  \n",
    "When n = 1, it is a 1-gram model or unigram model, which means that the appearing probability of the current word does not depend on any other words but itself. A unigram model can be treated as the combination of several one-state finite automata. It splits the probabilities of different terms in a context.   \n",
    "e.g. from $P(t_1t_2t_3)=P(t_1)P(t_2 \\mid t_1)P(t_3 \\mid t_1t_2)$ to $P_{uni}(t_1t_2t_3)=P(t_1)P(t_2)P(t_3)$.\n",
    "$$P_{uni}(w_1,... ,w_m)=\\prod_{i=1}^{m} P(w_i)$$\n",
    "\n",
    "In this model, the probability of each word only depends on that word's own probability in the document, so we only have one-state finite automata as units. The automaton itself has a probability distribution over the entire vocabulary of the model, summing to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "Advantages:  \n",
    "1. Using maximum likelihood estimation, parameters are easy to train.  \n",
    "2. completely contains all the information of its nearest previous word, which is strongly binding on the emergence of the current word.  \n",
    "3. Strong interpretability, intuitive and easy to understand.\n",
    "4. Since n = 1, the parameter space is not very large.\n",
    "\n",
    "Disadvantages:  \n",
    "1. Lack of long-term dependence, can only model the current word itself.  \n",
    "2. As n = 1, the accuracy may not be very high since it assumes that the occurrence of the current does not depend on other words although it is not true in most cases. For example, giving a sentence \"I had a great &lt;breakfast/dinner&gt; last night.\" In this case, the word in the angle brackets is related to the word \"night\" which is the last word in the sentence. \n",
    "3. Data is sparse, and the OOV (out-of-vocabulary) problem will inevitably occur.  \n",
    "4. Simple based on statistical frequency, poor generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:  \n",
    "When n = 2, the n-gram model is also called a bigram model, which assumes that the occurring probability of the current word depends on one previous word. The probability can be calculated as:  \n",
    "\n",
    "$$P_{bin}(w_1,... ,w_m)=\\prod_{i=1}^{m} P(w_i \\mid w_{i-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    "> \n",
    ">\n",
    "\n",
    "![WstWorld](https://timgsa.baidu.com/timg?image&quality=80&size=b10000_10000&sec=1561818705&di=95ca9ff2ff37fcb88ae47b82c7079feb&src=http://s7.sinaimg.cn/mw690/006BKUGwzy75VK46FMi66&690)\n",
    "\n",
    "> \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = '''\n",
    "sentence = 寒暄 介绍 询问 主题 结尾 \n",
    "介绍 = 我是 学生 ,\n",
    "学生 = 哈利 | 罗恩 | 赫敏 | 金妮 | 马尔福 | 卢娜\n",
    "寒暄 = 打招呼 , 老师 ,\n",
    "老师 = 人名 人称\n",
    "人名 = 邓布利多 | 斯内普 | 麦格 | 穆迪\n",
    "人称 = 教授\n",
    "打招呼 = 嗨 | 您好 \n",
    "询问 = 请问您能告诉我 | 您知道\n",
    "主题 = 福灵剂的配方 | 隐形斗篷 | 死亡圣器 | 格兰芬多之剑\n",
    "疑问 = 在哪里 | 的事情\n",
    "结尾 = 疑问 吗？\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = '''\n",
    "sentence = 寒暄 答复\n",
    "寒暄 = 噢 , 打招呼 ,\n",
    "打招呼 = 你好 | 是你呀 \n",
    "答复 = 抱歉 , 我不清楚这件事 。 | 是的 , 我知道 , 你要找的东西就在 地点 里面 。\n",
    "地点 = 古灵阁的地下金库 | 阿兹卡班的地牢 | 凤凰社的阁楼 |海格的小屋\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'您好,斯内普教授,我是马尔福,请问您能告诉我隐形斗篷在哪里吗？'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(gram=create_grammar(student, split='='), target='sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'噢,你好,是的,我知道,你要找的东西就在阿兹卡班的地牢里面。'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(gram=create_grammar(teacher, split='='), target='sentence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n(n, object):\n",
    "    gram = create_grammar(object, split='=')\n",
    "    target = 'sentence'\n",
    "    sentence_list = []\n",
    "    for i in range(n):\n",
    "        sentence_list.append(generate(gram, target))\n",
    "    return sentence_list\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['您好,邓布利多教授,我是金妮,您知道死亡圣器在哪里吗？',\n",
       " '嗨,穆迪教授,我是赫敏,您知道格兰芬多之剑的事情吗？',\n",
       " '嗨,穆迪教授,我是金妮,请问您能告诉我隐形斗篷的事情吗？',\n",
       " '您好,邓布利多教授,我是卢娜,您知道福灵剂的配方在哪里吗？',\n",
       " '您好,麦格教授,我是罗恩,您知道死亡圣器在哪里吗？']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n(5, student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'movie_comments.csv'\n",
    "content = pd.read_csv(filename, encoding='utf-8')\n",
    "content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = content['comment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    return re.findall('\\w+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_clean = [''.join(token(str(c))) for c in comments]\n",
    "len(comments_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string): \n",
    "    return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_jieba_cut = Counter(jieba.cut(comments[150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('，', 8),\n",
       " ('。', 5),\n",
       " ('的', 5),\n",
       " ('电影', 4),\n",
       " ('是', 3),\n",
       " ('这部', 3),\n",
       " ('对', 3),\n",
       " ('经典', 2),\n",
       " ('有', 2),\n",
       " ('看', 2)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_jieba_cut.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'经典能成为经典是有卓越之处看这部电影心中蒙着一层暖意篮球场上父子的对话很戳人心父亲之所以这么拼命是出于对自己的信心也是出于对孩子对家庭的责任感吧看了这部电影突然就有了勇气去闯去拼搏好的电影亮点肯定不止一个这部电影赢得我青睐赚得我泪点的地方在于父子情拼搏气'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(token(comments[150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comments_mv.txt', 'w') as f:\n",
    "    for c in comments_clean:\n",
    "        f.write(c + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate((open('comments_mv.txt'))):\n",
    "    if i % 10000 == 0: print(i)\n",
    "    if i > len(comments_clean): break\n",
    "    TOKEN += cut(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 1133810),\n",
       " ('\\n', 903459),\n",
       " ('了', 359285),\n",
       " ('是', 254517),\n",
       " ('我', 175377),\n",
       " ('都', 126734),\n",
       " ('很', 119814),\n",
       " ('看', 118557),\n",
       " ('电影', 116777),\n",
       " ('也', 111823),\n",
       " ('和', 108606),\n",
       " ('在', 107973),\n",
       " ('不', 99067),\n",
       " ('有', 97335),\n",
       " ('就', 90086),\n",
       " ('人', 82686),\n",
       " ('好', 79463),\n",
       " ('啊', 72839),\n",
       " ('还', 61444),\n",
       " ('这', 61056),\n",
       " ('你', 60531),\n",
       " ('一个', 60116),\n",
       " ('还是', 57488),\n",
       " ('但', 54199),\n",
       " ('故事', 51624),\n",
       " ('没有', 49986),\n",
       " ('就是', 49037),\n",
       " ('喜欢', 46668),\n",
       " ('让', 46292),\n",
       " ('太', 44157),\n",
       " ('剧情', 40657),\n",
       " ('又', 40042),\n",
       " ('没', 38166),\n",
       " ('说', 37793),\n",
       " ('吧', 37636),\n",
       " ('他', 37052),\n",
       " ('不错', 36608),\n",
       " ('给', 36243),\n",
       " ('到', 36227),\n",
       " ('得', 36069),\n",
       " ('上', 35122),\n",
       " ('这个', 35057),\n",
       " ('被', 34575),\n",
       " ('一部', 34025),\n",
       " ('对', 33932),\n",
       " ('最后', 33915),\n",
       " ('什么', 33430),\n",
       " ('能', 33410),\n",
       " ('片子', 33002),\n",
       " ('多', 31537),\n",
       " ('可以', 31318),\n",
       " ('与', 31145),\n",
       " ('不是', 30684),\n",
       " ('最', 30284),\n",
       " ('觉得', 30116),\n",
       " ('导演', 29145),\n",
       " ('中', 28958),\n",
       " ('自己', 28717),\n",
       " ('好看', 28597),\n",
       " ('拍', 28339),\n",
       " ('要', 28225),\n",
       " ('真的', 27802),\n",
       " ('感觉', 27152),\n",
       " ('但是', 26890),\n",
       " ('里', 26495),\n",
       " ('那', 25995),\n",
       " ('这部', 25941),\n",
       " ('有点', 25941),\n",
       " ('想', 25914),\n",
       " ('会', 25776),\n",
       " ('去', 25514),\n",
       " ('把', 25003),\n",
       " ('着', 24258),\n",
       " ('这么', 23797),\n",
       " ('个', 22919),\n",
       " ('小', 22758),\n",
       " ('真是', 22479),\n",
       " ('那么', 22358),\n",
       " ('这样', 22276),\n",
       " ('而', 22233),\n",
       " ('这种', 22175),\n",
       " ('不过', 22036),\n",
       " ('片', 22021),\n",
       " ('挺', 21799),\n",
       " ('时候', 21561),\n",
       " ('比', 21445),\n",
       " ('更', 21373),\n",
       " ('大', 20370),\n",
       " ('却', 20351),\n",
       " ('我们', 20188),\n",
       " ('爱', 20070),\n",
       " ('演技', 19974),\n",
       " ('虽然', 19711),\n",
       " ('像', 19602),\n",
       " ('其实', 19348),\n",
       " ('看到', 18986),\n",
       " ('再', 18836),\n",
       " ('知道', 18801),\n",
       " ('演员', 18589),\n",
       " ('来', 17763)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count = Counter(TOKEN)\n",
    "words_count.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies = [f for w, f in words_count.most_common(100)]\n",
    "x = [i for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_1(word):\n",
    "    return words_count[word]/len(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.864270435563828e-06"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_1('战狼')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吴京', '意淫', '到', '了', '脑残', '的', '地步', '看', '了', '恶心']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = [str(t) for t in TOKEN]\n",
    "TOKEN_2_GRAM = [''.join(TOKEN[i:i+2]) for i in range(len(TOKEN[:-2]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['吴京意淫', '意淫到', '到了', '了脑残', '脑残的', '的地步', '地步看', '看了', '了恶心', '恶心想']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN_2_GRAM[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count_2 = Counter(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_2(word1, word2):\n",
    "    if word1+word2 in words_count_2:\n",
    "        return words_count_2[word1+word2]/len(TOKEN_2_GRAM)\n",
    "    else:\n",
    "        return 1 / len(TOKEN_2_GRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3876861359610325e-06"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_2('战狼', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(sentence):\n",
    "    words = cut(sentence)\n",
    "    sentence_pro = 1\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        next_ = words[i + 1]\n",
    "        probability = prob_2(word, next_)\n",
    "        sentence_pro *= probability\n",
    "    return sentence_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2532902279602501e-118"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probability('您好,邓布利多教授,我是金妮,您知道死亡圣器在哪里吗？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: 嗨,邓布利多教授,我是卢娜,请问您能告诉我福灵剂的配方的事情吗？ with Prb: 2.7456613621463463e-134\n",
      "sentence: 您好,麦格教授,我是卢娜,您知道格兰芬多之剑在哪里吗？ with Prb: 1.5685735018275983e-121\n",
      "sentence: 您好,穆迪教授,我是赫敏,您知道福灵剂的配方的事情吗？ with Prb: 7.19949340819999e-112\n",
      "sentence: 嗨,麦格教授,我是罗恩,您知道福灵剂的配方在哪里吗？ with Prb: 1.9913653880294804e-111\n",
      "sentence: 您好,穆迪教授,我是哈利,请问您能告诉我格兰芬多之剑的事情吗？ with Prb: 1.9082863291921828e-129\n"
     ]
    }
   ],
   "source": [
    "for sen in [generate(gram=create_grammar(student, split='='), target='sentence') for i in range(5)]:\n",
    "    print('sentence: {} with Prb: {}'.format(sen, get_probability(sen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "哈利波特买了一根魔杖 is more possible\n",
      "---- 哈利波特买了一根魔杖 with probability 3.883815199476545e-28\n",
      "---- 魔杖买了一根哈利波特 with probability 3.609381314771786e-30\n",
      "那只猫头鹰名字叫海德薇 is more possible\n",
      "---- 那只猫头鹰名字叫海德薇 with probability 6.361587882892599e-38\n",
      "---- 那只猫头鹰明字叫海德薇 with probability 5.089270306314079e-40\n",
      "摄魂怪有很多阿兹卡班监狱看守 is more possible\n",
      "---- 阿兹卡班监狱有很多摄魂怪看守 with probability 6.462059802460623e-40\n",
      "---- 摄魂怪有很多阿兹卡班监狱看守 with probability 3.5606933898667944e-33\n",
      "哈利昨天晚上去了森林 is more possible\n",
      "---- 哈利昨天晚上去了森林 with probability 1.9751209159268997e-25\n",
      "---- 哈利明天晚上去了森林 with probability 6.691089743732214e-31\n"
     ]
    }
   ],
   "source": [
    "need_compared = [\n",
    "    \"哈利波特买了一根魔杖 魔杖买了一根哈利波特\",\n",
    "    \"那只猫头鹰名字叫海德薇 那只猫头鹰明字叫海德薇\",\n",
    "    \"阿兹卡班监狱有很多摄魂怪看守 摄魂怪有很多阿兹卡班监狱看守\",\n",
    "    \"哈利昨天晚上去了森林 哈利明天晚上去了森林\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = get_probability(s1), get_probability(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probability {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probability {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1, 3, 5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数接受一个参数key，这个参数接受一个函数作为输入，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第0个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0), (1, 4), (4, 4), (2, 5)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (1, 4), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序, 但是是递减的顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(): # you code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
